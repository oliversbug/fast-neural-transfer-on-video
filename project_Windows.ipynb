{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDcMjVvA6zLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from torchvision import models\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.onnx\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPSqA6A-6J7-",
        "colab_type": "text"
      },
      "source": [
        "Image Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE_MH93l553L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(filename, size=None, scale=None):\n",
        "    img = Image.open(filename)\n",
        "    if size is not None:\n",
        "        img = img.resize((size, size), Image.ANTIALIAS)\n",
        "    elif scale is not None:\n",
        "        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n",
        "    return img\n",
        "\n",
        "\n",
        "def save_image(filename, data):\n",
        "    img = data.clone().clamp(0, 255).numpy()\n",
        "    img = img.transpose(1, 2, 0).astype(\"uint8\")\n",
        "    img = Image.fromarray(img)\n",
        "    img.save(filename)\n",
        "\n",
        "\n",
        "def gram_matrix(y):\n",
        "    (b, ch, h, w) = y.size()\n",
        "    features = y.view(b, ch, w * h)\n",
        "    features_t = features.transpose(1, 2)\n",
        "    gram = features.bmm(features_t) / (ch * h * w)\n",
        "    return gram\n",
        "\n",
        "\n",
        "def normalize_batch(batch):\n",
        "    # normalize using imagenet mean and std\n",
        "    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "    std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "    batch = batch.div_(255.0)\n",
        "    return (batch - mean) / std\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjO9RBDL6bly",
        "colab_type": "text"
      },
      "source": [
        "VGG Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CNfRgah5-RT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vgg16(torch.nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg16, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        for x in range(4):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = self.slice1(X)\n",
        "        h_relu1_2 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3_3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4_3 = h\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'])\n",
        "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXNQT8OZ6TGN",
        "colab_type": "text"
      },
      "source": [
        "TransformerNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSFTosJEyHQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "        # Initial convolution layers\n",
        "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
        "        # Residual layers\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "        # Upsampling Layers\n",
        "        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
        "        # Non-linearities\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = self.relu(self.in1(self.conv1(X)))\n",
        "        y = self.relu(self.in2(self.conv2(y)))\n",
        "        y = self.relu(self.in3(self.conv3(y)))\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "        y = self.relu(self.in4(self.deconv1(y)))\n",
        "        y = self.relu(self.in5(self.deconv2(y)))\n",
        "        y = self.deconv3(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class ConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    \"\"\"ResidualBlock\n",
        "    introduced in: https://arxiv.org/abs/1512.03385\n",
        "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpsampleConvLayer(torch.nn.Module):\n",
        "    \"\"\"UpsampleConvLayer\n",
        "    Upsamples the input and then does a convolution. This method gives better results\n",
        "    compared to ConvTranspose2d.\n",
        "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoCrrGhD6nBH",
        "colab_type": "text"
      },
      "source": [
        "Main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0_5dEpx6hJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_paths(save_model_dir, checkpoint_model_dir = None):\n",
        "    try:\n",
        "        if not os.path.exists(save_model_dir):\n",
        "            os.makedirs(save_model_dir)\n",
        "        if checkpoint_model_dir is not None and not (os.path.exists(checkpoint_model_dir)):\n",
        "            os.makedirs(checkpoint_model_dir)\n",
        "    except OSError as e:\n",
        "        print(e)\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "def train(dataset,style_image,save_model_dir,cuda):\n",
        "    seed = 42\n",
        "    image_size = 256\n",
        "    style_size = None\n",
        "    batch_size = 4\n",
        "    lr = 1e-3\n",
        "    epochs = 2\n",
        "    content_weight = 1e5\n",
        "    style_weight = 1e10\n",
        "    log_interval = 500\n",
        "    checkpoint_model_dir = None\n",
        "    checkpoint_interval = 2000\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "    \n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    train_dataset = datasets.ImageFolder(dataset, transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "    transformer = TransformerNet().to(device)\n",
        "    optimizer = Adam(transformer.parameters(), lr)\n",
        "    mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "    vgg = Vgg16(requires_grad=False).to(device)\n",
        "    style_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    style = load_image(style_image, size=style_size)\n",
        "    style = style_transform(style)\n",
        "    style = style.repeat(batch_size, 1, 1, 1).to(device)\n",
        "\n",
        "    features_style = vgg(normalize_batch(style))\n",
        "    gram_style = [gram_matrix(y) for y in features_style]\n",
        "\n",
        "    for e in range(epochs):\n",
        "        transformer.train()\n",
        "        agg_content_loss = 0.\n",
        "        agg_style_loss = 0.\n",
        "        count = 0\n",
        "        for batch_id, (x, _) in enumerate(train_loader):\n",
        "            n_batch = len(x)\n",
        "            count += n_batch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = transformer(x)\n",
        "\n",
        "            y = normalize_batch(y)\n",
        "            x = normalize_batch(x)\n",
        "\n",
        "            features_y = vgg(y)\n",
        "            features_x = vgg(x)\n",
        "\n",
        "            content_loss = content_weight * mse_loss(features_y.relu2_2, features_x.relu2_2)\n",
        "\n",
        "            style_loss = 0.\n",
        "            for ft_y, gm_s in zip(features_y, gram_style):\n",
        "                gm_y = gram_matrix(ft_y)\n",
        "                style_loss += mse_loss(gm_y, gm_s[:n_batch, :, :])\n",
        "            style_loss *= style_weight\n",
        "\n",
        "            total_loss = content_loss + style_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            agg_content_loss += content_loss.item()\n",
        "            agg_style_loss += style_loss.item()\n",
        "\n",
        "            if (batch_id + 1) % log_interval == 0:\n",
        "                mesg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
        "                    time.ctime(), e + 1, count, len(train_dataset),\n",
        "                                  agg_content_loss / (batch_id + 1),\n",
        "                                  agg_style_loss / (batch_id + 1),\n",
        "                                  (agg_content_loss + agg_style_loss) / (batch_id + 1)\n",
        "                )\n",
        "                print(mesg)\n",
        "\n",
        "            if checkpoint_model_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n",
        "                transformer.eval().cpu()\n",
        "                ckpt_model_filename = \"ckpt_epoch_\" + str(e) + \"_batch_id_\" + str(batch_id + 1) + \".pth\"\n",
        "                ckpt_model_path = os.path.join(checkpoint_model_dir, ckpt_model_filename)\n",
        "                torch.save(transformer.state_dict(), ckpt_model_path)\n",
        "                transformer.to(device).train()\n",
        "\n",
        "    # save model\n",
        "    transformer.eval().cpu()\n",
        "    #save_model_filename = str(time.ctime()).replace(' ', '_') + \"model\"\n",
        "    #save_model_filename = r'{}'.format(save_model_filename)\n",
        "    #save_model_path = os.path.join(save_model_dir, save_model_filename)\n",
        "    #save_model_path = save_model_dir + r'\\t' + save_model_filename\n",
        "    #torch.save(transformer.state_dict(), save_model_path)\n",
        "    torch.save(transformer.state_dict(), 'Model/Style.model')\n",
        "\n",
        "    print(\"\\nDone, trained model saved\")\n",
        "\n",
        "\n",
        "def stylize(content_image, output_image, model, cuda):\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "    content_scale = 2\n",
        "    content_image = load_image(content_image, scale=content_scale)\n",
        "    content_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    content_image = content_transform(content_image)\n",
        "    content_image = content_image.unsqueeze(0).to(device)\n",
        "\n",
        "    # if model.endswith(\".onnx\"):\n",
        "    #     output = stylize_onnx_caffe2(content_image, args)\n",
        "    # else:\n",
        "    #     with torch.no_grad():\n",
        "    #         style_model = TransformerNet()\n",
        "    #         state_dict = torch.load(model)\n",
        "    #         # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
        "    #         for k in list(state_dict.keys()):\n",
        "    #             if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
        "    #                 del state_dict[k]\n",
        "    #         style_model.load_state_dict(state_dict)\n",
        "    #         style_model.to(device)\n",
        "    #         if export_onnx:\n",
        "    #             assert export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"\n",
        "    #             output = torch.onnx._export(style_model, content_image, export_onnx).cpu()\n",
        "    #         else:\n",
        "    #             output = style_model(content_image).cpu()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      style_model = TransformerNet()\n",
        "      state_dict = torch.load(model)\n",
        "      # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
        "      for k in list(state_dict.keys()):\n",
        "          if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
        "              del state_dict[k]\n",
        "      style_model.load_state_dict(state_dict)\n",
        "      style_model.to(device)\n",
        "      output = style_model(content_image).cpu()   \n",
        "    \n",
        "    #return output[0]\n",
        "    save_image(output_image, output[0])\n",
        "\n",
        "\n",
        "# def stylize_onnx_caffe2(content_image, model, cuda):\n",
        "#     \"\"\"\n",
        "#     Read ONNX model and run it using Caffe2\n",
        "#     \"\"\"\n",
        "\n",
        "#     assert not args.export_onnx\n",
        "\n",
        "#     import onnx\n",
        "#     import onnx_caffe2.backend\n",
        "\n",
        "#     model = onnx.load(model)\n",
        "\n",
        "#     prepared_backend = onnx_caffe2.backend.prepare(model, device='CUDA' if cuda else 'CPU')\n",
        "#     inp = {model.graph.input[0].name: content_image.numpy()}\n",
        "#     c2_out = prepared_backend.run(inp)[0]\n",
        "\n",
        "#     return torch.from_numpy(c2_out)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52wuULzFXQY8",
        "colab_type": "code",
        "outputId": "c506b5c1-069d-46c4-f611-b6035f980d97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407,
          "referenced_widgets": [
            "ee5997e93d5644d890a31d2c9b1b42d4",
            "954351bf999a48e6acc8ceaa7b6f533b",
            "1faa9f8484ad4a11b6c299ed760f1720",
            "35198a3d3b37480db34a9a19a7793c37",
            "1ff5f2a39e4f49b9835db5cc49ccf1f1",
            "a48f9017af60470796be87cbe4043d4b",
            "94a5bf09540449e69488f1e362ed2db7",
            "e9079432800149279b3d2d359b9c8b67"
          ]
        }
      },
      "source": [
        "dataset = 'val2017'        #COCO 2017 Datasets mounted on My Google Drive\n",
        "style_image = 'style/Composition-VIII.jpg'\n",
        "save_model_dir = 'Model'\n",
        "cuda = 1\n",
        "check_paths(save_model_dir)\n",
        "train(dataset,style_image,save_model_dir,cuda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fa-Kp4qAGsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = 'mosaic.pth'\n",
        "#model = 'test.model'\n",
        "cuda = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMaNICMXDbmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "video_path = 'Clip5.mp4'\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "count = 0\n",
        "frames = []\n",
        "bests = []\n",
        "new_frames = []\n",
        "while True:\n",
        "    _, frame = cap.read()\n",
        "    if frame is None:\n",
        "        break\n",
        "    \n",
        "    cv2.imwrite('tmp.jpg', frame)\n",
        "    #frame = cv2.resize(frame,(256, 256),cv2.INTER_AREA)\n",
        "    stylize('tmp.jpg', 'tmp2.jpg', model, cuda)\n",
        "    tmp = cv2.imread('tmp2.jpg')\n",
        "    new_frames.append(tmp)\n",
        "     \n",
        "    count += 1\n",
        "    if count%10 == 0:\n",
        "      print('Frame:{}'.format(count))\n",
        "#------------------    \n",
        "cap.release()\n",
        "print(count)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z7oD6YKDjSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fps = 30\n",
        "shape = new_frames[0].shape\n",
        "video_size = (shape[1],shape[0])\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "video_writer = cv2.VideoWriter(filename='result.mp4',fourcc=fourcc, fps=fps,frameSize=video_size)\n",
        "for frame in new_frames:\n",
        "  video_writer.write(frame)\n",
        "#cv2.waitKey(100)\n",
        "\n",
        "video_writer.release()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGbasI7AxyHE",
        "colab_type": "code",
        "outputId": "f4884355-8d9c-4875-a57d-389fb2ba8daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "frame.shape"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "CV Project 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ee5997e93d5644d890a31d2c9b1b42d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_954351bf999a48e6acc8ceaa7b6f533b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1faa9f8484ad4a11b6c299ed760f1720",
              "IPY_MODEL_35198a3d3b37480db34a9a19a7793c37"
            ]
          }
        },
        "954351bf999a48e6acc8ceaa7b6f533b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1faa9f8484ad4a11b6c299ed760f1720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1ff5f2a39e4f49b9835db5cc49ccf1f1",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a48f9017af60470796be87cbe4043d4b"
          }
        },
        "35198a3d3b37480db34a9a19a7793c37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_94a5bf09540449e69488f1e362ed2db7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:05&lt;00:00, 107MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9079432800149279b3d2d359b9c8b67"
          }
        },
        "1ff5f2a39e4f49b9835db5cc49ccf1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a48f9017af60470796be87cbe4043d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "94a5bf09540449e69488f1e362ed2db7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9079432800149279b3d2d359b9c8b67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}