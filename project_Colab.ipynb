{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CV Project 3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDcMjVvA6zLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "from torchvision import models\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.onnx\n",
        "\n",
        "#import argparse\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPSqA6A-6J7-",
        "colab_type": "text"
      },
      "source": [
        "#Image Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE_MH93l553L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_image(filename, size=None, scale=None):\n",
        "    img = Image.open(filename)\n",
        "    if size is not None:\n",
        "        img = img.resize((size, size), Image.ANTIALIAS)\n",
        "    elif scale is not None:\n",
        "        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n",
        "    return img\n",
        "\n",
        "\n",
        "def save_image(filename, data):\n",
        "    img = data.clone().clamp(0, 255).numpy()\n",
        "    img = img.transpose(1, 2, 0).astype(\"uint8\")\n",
        "    img = Image.fromarray(img)\n",
        "    img.save(filename)\n",
        "\n",
        "\n",
        "def gram_matrix(y):\n",
        "    (b, ch, h, w) = y.size()\n",
        "    features = y.view(b, ch, w * h)\n",
        "    features_t = features.transpose(1, 2)\n",
        "    gram = features.bmm(features_t) / (ch * h * w)\n",
        "    return gram\n",
        "\n",
        "\n",
        "def normalize_batch(batch):\n",
        "    # normalize using imagenet mean and std\n",
        "    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "    std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "    batch = batch.div_(255.0)\n",
        "    return (batch - mean) / std\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjO9RBDL6bly",
        "colab_type": "text"
      },
      "source": [
        "#VGG Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CNfRgah5-RT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vgg16(torch.nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super(Vgg16, self).__init__()\n",
        "        vgg_pretrained_features = models.vgg16(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        for x in range(4):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = self.slice1(X)\n",
        "        h_relu1_2 = h\n",
        "        h = self.slice2(h)\n",
        "        h_relu2_2 = h\n",
        "        h = self.slice3(h)\n",
        "        h_relu3_3 = h\n",
        "        h = self.slice4(h)\n",
        "        h_relu4_3 = h\n",
        "        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3'])\n",
        "        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXNQT8OZ6TGN",
        "colab_type": "text"
      },
      "source": [
        "#TransformerNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSFTosJEyHQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerNet, self).__init__()\n",
        "        # Initial convolution layers\n",
        "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
        "        self.in3 = torch.nn.InstanceNorm2d(128, affine=True)\n",
        "        # Residual layers\n",
        "        self.res1 = ResidualBlock(128)\n",
        "        self.res2 = ResidualBlock(128)\n",
        "        self.res3 = ResidualBlock(128)\n",
        "        self.res4 = ResidualBlock(128)\n",
        "        self.res5 = ResidualBlock(128)\n",
        "        # Upsampling Layers\n",
        "        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in4 = torch.nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n",
        "        self.in5 = torch.nn.InstanceNorm2d(32, affine=True)\n",
        "        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n",
        "        # Non-linearities\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, X):\n",
        "        y = self.relu(self.in1(self.conv1(X)))\n",
        "        y = self.relu(self.in2(self.conv2(y)))\n",
        "        y = self.relu(self.in3(self.conv3(y)))\n",
        "        y = self.res1(y)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y = self.res4(y)\n",
        "        y = self.res5(y)\n",
        "        y = self.relu(self.in4(self.deconv1(y)))\n",
        "        y = self.relu(self.in5(self.deconv2(y)))\n",
        "        y = self.deconv3(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class ConvLayer(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "        super(ConvLayer, self).__init__()\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.reflection_pad(x)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResidualBlock(torch.nn.Module):\n",
        "    \"\"\"ResidualBlock\n",
        "    introduced in: https://arxiv.org/abs/1512.03385\n",
        "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
        "        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.in1(self.conv1(x)))\n",
        "        out = self.in2(self.conv2(out))\n",
        "        out = out + residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class UpsampleConvLayer(torch.nn.Module):\n",
        "    \"\"\"UpsampleConvLayer\n",
        "    Upsamples the input and then does a convolution. This method gives better results\n",
        "    compared to ConvTranspose2d.\n",
        "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
        "        super(UpsampleConvLayer, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        reflection_padding = kernel_size // 2\n",
        "        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n",
        "        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_in = x\n",
        "        if self.upsample:\n",
        "            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
        "        out = self.reflection_pad(x_in)\n",
        "        out = self.conv2d(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoCrrGhD6nBH",
        "colab_type": "text"
      },
      "source": [
        "#Main function()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0_5dEpx6hJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_paths(save_model_dir, checkpoint_model_dir = None):\n",
        "    try:\n",
        "        if not os.path.exists(save_model_dir):\n",
        "            os.makedirs(save_model_dir)\n",
        "        if checkpoint_model_dir is not None and not (os.path.exists(checkpoint_model_dir)):\n",
        "            os.makedirs(checkpoint_model_dir)\n",
        "    except OSError as e:\n",
        "        print(e)\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "def train(dataset,style_image,save_model_dir,cuda):\n",
        "    seed = 42\n",
        "    image_size = 256\n",
        "    style_size = None\n",
        "    batch_size = 4\n",
        "    lr = 1e-3\n",
        "    epochs = 2\n",
        "    content_weight = 1e5\n",
        "    style_weight = 1e10\n",
        "    log_interval = 500\n",
        "    checkpoint_model_dir = None\n",
        "    checkpoint_interval = 2000\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "    \n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.CenterCrop(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    train_dataset = datasets.ImageFolder(dataset, transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "    transformer = TransformerNet().to(device)\n",
        "    optimizer = Adam(transformer.parameters(), lr)\n",
        "    mse_loss = torch.nn.MSELoss()\n",
        "\n",
        "    vgg = Vgg16(requires_grad=False).to(device)\n",
        "    style_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    style = load_image(style_image, size=style_size)\n",
        "    style = style_transform(style)\n",
        "    style = style.repeat(batch_size, 1, 1, 1).to(device)\n",
        "\n",
        "    features_style = vgg(normalize_batch(style))\n",
        "    gram_style = [gram_matrix(y) for y in features_style]\n",
        "\n",
        "    for e in range(epochs):\n",
        "        transformer.train()\n",
        "        agg_content_loss = 0.\n",
        "        agg_style_loss = 0.\n",
        "        count = 0\n",
        "        for batch_id, (x, _) in enumerate(train_loader):\n",
        "            n_batch = len(x)\n",
        "            count += n_batch\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x = x.to(device)\n",
        "            y = transformer(x)\n",
        "\n",
        "            y = normalize_batch(y)\n",
        "            x = normalize_batch(x)\n",
        "\n",
        "            features_y = vgg(y)\n",
        "            features_x = vgg(x)\n",
        "\n",
        "            content_loss = content_weight * mse_loss(features_y.relu2_2, features_x.relu2_2)\n",
        "\n",
        "            style_loss = 0.\n",
        "            for ft_y, gm_s in zip(features_y, gram_style):\n",
        "                gm_y = gram_matrix(ft_y)\n",
        "                style_loss += mse_loss(gm_y, gm_s[:n_batch, :, :])\n",
        "            style_loss *= style_weight\n",
        "\n",
        "            total_loss = content_loss + style_loss\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            agg_content_loss += content_loss.item()\n",
        "            agg_style_loss += style_loss.item()\n",
        "\n",
        "            if (batch_id + 1) % log_interval == 0:\n",
        "                mesg = \"{}\\tEpoch {}:\\t[{}/{}]\\tcontent: {:.6f}\\tstyle: {:.6f}\\ttotal: {:.6f}\".format(\n",
        "                    time.ctime(), e + 1, count, len(train_dataset),\n",
        "                                  agg_content_loss / (batch_id + 1),\n",
        "                                  agg_style_loss / (batch_id + 1),\n",
        "                                  (agg_content_loss + agg_style_loss) / (batch_id + 1)\n",
        "                )\n",
        "                print(mesg)\n",
        "\n",
        "            if checkpoint_model_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n",
        "                transformer.eval().cpu()\n",
        "                ckpt_model_filename = \"ckpt_epoch_\" + str(e) + \"_batch_id_\" + str(batch_id + 1) + \".pth\"\n",
        "                ckpt_model_path = os.path.join(checkpoint_model_dir, ckpt_model_filename)\n",
        "                torch.save(transformer.state_dict(), ckpt_model_path)\n",
        "                transformer.to(device).train()\n",
        "\n",
        "    # save model\n",
        "    transformer.eval().cpu()\n",
        "    save_model_filename = str(time.ctime()).replace(' ', '_') + \".model\"\n",
        "    save_model_path = os.path.join(save_model_dir, save_model_filename)\n",
        "    torch.save(transformer.state_dict(), save_model_path)\n",
        "\n",
        "    print(\"\\nDone, trained model saved at\", save_model_path)\n",
        "\n",
        "\n",
        "def stylize(content_image, output_image, model, cuda, content_scale = None):\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "    content_image = load_image(content_image, scale=content_scale)\n",
        "    content_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x.mul(255))\n",
        "    ])\n",
        "    content_image = content_transform(content_image)\n",
        "    content_image = content_image.unsqueeze(0).to(device)\n",
        "\n",
        "    # if model.endswith(\".onnx\"):\n",
        "    #     output = stylize_onnx_caffe2(content_image, model, cuda)\n",
        "    # else:\n",
        "    #     with torch.no_grad():\n",
        "    #         style_model = TransformerNet()\n",
        "    #         state_dict = torch.load(model)\n",
        "    #         # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
        "    #         for k in list(state_dict.keys()):\n",
        "    #             if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
        "    #                 del state_dict[k]\n",
        "    #         style_model.load_state_dict(state_dict)\n",
        "    #         style_model.to(device)\n",
        "    #         # if export_onnx:\n",
        "    #         #     assert export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"\n",
        "    #         #     output = torch.onnx._export(style_model, content_image, export_onnx).cpu()\n",
        "    #         # else:\n",
        "    #         #     output = style_model(content_image).cpu()\n",
        "    #         output = style_model(content_image).cpu()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      style_model = TransformerNet()\n",
        "      state_dict = torch.load(model)\n",
        "      # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
        "      for k in list(state_dict.keys()):\n",
        "          if re.search(r'in\\d+\\.running_(mean|var)$', k):\n",
        "              del state_dict[k]\n",
        "      style_model.load_state_dict(state_dict)\n",
        "      style_model.to(device)\n",
        "      output = style_model(content_image).cpu()   \n",
        "    \n",
        "    save_image(output_image, output[0])\n",
        "\n",
        "\n",
        "# def stylize_onnx_caffe2(content_image, model, cuda):\n",
        "#     \"\"\"\n",
        "#     Read ONNX model and run it using Caffe2\n",
        "#     \"\"\"\n",
        "\n",
        "#     #assert not args.export_onnx\n",
        "#     !pip install onnx\n",
        "#     !pip install caffe2.python.onnx\n",
        "#     import onnx\n",
        "#     import caffe2.python.onnx.backend as backend\n",
        "\n",
        "#     model = onnx.load(model)\n",
        "\n",
        "#     prepared_backend = backend.prepare(model, device='CUDA' if cuda else 'CPU')\n",
        "#     inp = {model.graph.input[0].name: content_image.numpy()}\n",
        "#     c2_out = prepared_backend.run(inp)[0]\n",
        "\n",
        "#     return torch.from_numpy(c2_out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbu3irhvMyaZ",
        "colab_type": "text"
      },
      "source": [
        "#Train Custom Transfer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52wuULzFXQY8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = 'drive/My Drive/TEST/val2017'        #COCO 2017 Datasets mounted on My Google Drive\n",
        "style_image = 'picasso.jpg'\n",
        "save_model_dir = 'Model'\n",
        "cuda = 1\n",
        "check_paths(save_model_dir)\n",
        "train(dataset,style_image,save_model_dir,cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U47G75dzM5DA",
        "colab_type": "text"
      },
      "source": [
        "#Stylize Image or Video\n",
        "## Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fa-Kp4qAGsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = 'Composition-VII.model'      #support .model, .pth\n",
        "style_name = model.split('.')[0]\n",
        "cuda = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSif_m2yMqDE",
        "colab_type": "text"
      },
      "source": [
        "## Image Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnI8ddkU21s7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Content_IMG = 'dancing.jpg'\n",
        "Output_IMG = '{}_styled.jpg'.format(Content_IMG.split('.')[0])\n",
        "stylize(Content_IMG, Output_IMG, model, cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX1py0fETlg4",
        "colab_type": "text"
      },
      "source": [
        "## Video Style Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MtFOiX-U4A1",
        "colab_type": "text"
      },
      "source": [
        "### Style Transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMaNICMXDbmE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "51c516ce-b305-42d2-a0fc-76128cd3343c"
      },
      "source": [
        "import cv2\n",
        "video_path = 'Clip2.mp4'\n",
        "video_name = video_path.split('.')[0]\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "original_folder = '{}_{}/Original'.format(video_name,style_name)\n",
        "styled_folder = '{}_{}/Styled'.format(video_name,style_name)\n",
        "check_paths(original_folder)\n",
        "check_paths(styled_folder)\n",
        "\n",
        "\n",
        "count = 0\n",
        "new_frames = []\n",
        "while True:\n",
        "    _, frame = cap.read()\n",
        "    if frame is None:\n",
        "        break\n",
        "    cv2.imwrite(os.path.join(original_folder, 'Frame_{}.jpg'.format(count)), frame)\n",
        "    #cv2.imwrite('Frame_{}.jpg'.format(count), frame)\n",
        "    stylize(os.path.join(original_folder, 'Frame_{}.jpg'.format(count)),os.path.join(styled_folder, 'Frame_{}.jpg'.format(count)), model, cuda)\n",
        "    \n",
        "    #stylize('{}\\Original\\Frame_{}.jpg'.format(video_name, count), '{}\\Styled\\Frame_{}.jpg'.format(video_name, count), model, cuda)\n",
        "    tmp = cv2.imread(os.path.join(styled_folder, 'Frame_{}.jpg'.format(count)))\n",
        "    new_frames.append(tmp)\n",
        "     \n",
        "    count += 1\n",
        "    if count%10 == 0:\n",
        "      print('Frame:{}'.format(count))\n",
        "#------------------    \n",
        "cap.release()\n",
        "print(count)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frame:10\n",
            "Frame:20\n",
            "Frame:30\n",
            "Frame:40\n",
            "Frame:50\n",
            "Frame:60\n",
            "Frame:70\n",
            "Frame:80\n",
            "Frame:90\n",
            "Frame:100\n",
            "Frame:110\n",
            "Frame:120\n",
            "Frame:130\n",
            "Frame:140\n",
            "Frame:150\n",
            "Frame:160\n",
            "Frame:170\n",
            "Frame:180\n",
            "Frame:190\n",
            "Frame:200\n",
            "Frame:210\n",
            "Frame:220\n",
            "Frame:230\n",
            "Frame:240\n",
            "Frame:250\n",
            "Frame:260\n",
            "Frame:270\n",
            "Frame:280\n",
            "Frame:290\n",
            "Frame:300\n",
            "Frame:310\n",
            "Frame:320\n",
            "Frame:330\n",
            "Frame:340\n",
            "Frame:350\n",
            "Frame:360\n",
            "Frame:370\n",
            "Frame:380\n",
            "Frame:390\n",
            "Frame:400\n",
            "Frame:410\n",
            "414\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YeK6FMeU7aL",
        "colab_type": "text"
      },
      "source": [
        "### Combine to video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z7oD6YKDjSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fps = 30\n",
        "shape = new_frames[0].shape\n",
        "resolution = (shape[1],shape[0])\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "video_writer = cv2.VideoWriter(filename='{}_{}/styledVideo.mp4'.format(video_name, style_name),fourcc=fourcc, fps=fps,frameSize=resolution)\n",
        "for frame in new_frames:\n",
        "  video_writer.write(frame)\n",
        "#cv2.waitKey(100)\n",
        "\n",
        "video_writer.release()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_dLune4quDo",
        "colab_type": "text"
      },
      "source": [
        "### Resolution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGbasI7AxyHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resolution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB33ZyF1qzLj",
        "colab_type": "text"
      },
      "source": [
        "## Zip Result Files to Download\n",
        "(Need to mannually modify the name every time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKvCeacnovUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!zip 'Clip2_Composition-VIII.zip' 'Clip2_Composition-VIII' -r    # Need to manually change the Directory Name in codes\n",
        "print('Done')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}